hashMap 的工作原理

你有使用过hashMap吗 ？ hashMap的特性， hashmap 没有使用 同步锁， synchronized 所以它很快，但是在多线程情况下多个线程同时达到负载因子 （0.75）重新扩容
（翻倍）时，会出现竞态条件。 会有线程安全问题。

hashMap 使用 hashing 的原理，我们使用 put(key ,value)存储对象到hashMap 中，使用 get(key) 从hashMap 中获取value.
当使用put方法传递键和值时，优先会调用键的 hashCode()方法,返回的hashCode 用于确定 bucket位置存储Entry. 

当两个对象的hashcode 相同时会发生什么，
因为有相同的hashcode ,所以会得到相同的 bucket位置，出现冲突。 hashMap处理这种冲突的方式是，追加在链表后。

如果两个键的hashCode 相同，如何获取值对象？
找到bucket 位置之后调用 keys.equals()方法去找到链表中正确的节点。



hashMap 和 hashtable 的区别
hashMap 和 HashTable都实现了Map 接口， 都可以当做Map 使用。 它们之间的区别主要有：
 
                                               HashMap                  hashTable                   concurrentHashMap
 线程安全性                                       非                       是                              是
 同步( synchronization )                          非 synchronized         是 synchronized                  
 速度（不需要同步，单线程情况下）                   快                      慢                          

1、synchronized （同步锁） 意味着一次仅仅只有一个线程可以访问更改 hashTable。 就是说任何线程要更新 hashTable时要首先获得同步锁，
其他线程必须要要等待同步锁被释放之后，才能再次获得同步锁更新HashTable
2、HashMap 也可以通过 Map  m = Collections.synchronizeMap(hashMap) 来实现同步

Java 5及以上，使用 concurrentHsahMap 替代 hashTable

函数式编程
一个输入，一个输出，数据不变性，多核高并发情况下

hashmap, 扩容过程
创建两倍大小的桶，然后把原来元素移到新桶中。


kafka 存储机制 cluster ,broker, topic ,partition ,replication, ISR in_sync_Replication, customer, producer
kafka

ES  简单使用

分布式事务
事务消息，tcc数据一致性最高


mysql 和 oracle 的事务隔离级别机制
mysql 支持的事务隔离界别有四个，未提交读，提交读，可重复读，串行
默认隔离级别是可重复读， 可重复读解决的问题是不可重复读。 这个问题简单的描述下是并发一致性问题中的一种场景 。
有两个事务A，B，A读取数据两次，在两次读取的间隔中，B修改了读取的数据。导致事务A中两次读取结果不一致。
直观的解决方案是使用悲观锁理论，在A事务中，第一次读取时，就对数据加行锁
MySQL采用的是乐观锁理论，采用mvcc + next-key 实现，解决不可重复读问题。
在每行数据后隐式的加入两列，一列记录创建时候的事务版本号，一列记录删除版本号。（就是记录了这行数据的生命周期）
事务版本号要解释一下记录的不是时间，是事务创建版本号就会递增。为什么不是时间？ 先发生后提交的问题。
在可重读隔离级别下，
select 读取创建版本号<= 当前事务版本号， 当前事务版本号 < 删除版本号 ，删除版本号 = 0 （读取当前事务号之前有效的数据）
insert 插入一条新记录，当前事务版本号记录为创建版本号
delete 修改删除版本号为当前事务版本号
update 插入一条新数据，当前版本号为事务创建版本号，同时修改原记录的删除版本号为当前版本号

通过mvcc 虽然多使用了一些存储空间，但可以减少锁的使用，大多数读操作都不用加锁，读数据操作很简单，性能很好。









